# CHANGELOG - Version 8.0

## Critical Fixes (v7 ‚Üí v8)

Version 8 addresses **three critical bugs** discovered in v7 that would have corrupted training data and posterior inference.

---

## üî¥ CRITICAL FIX #1: Per-Population Bottleneck Parameters Not Transformed

### The Bug (v7)
The transformation logic in `transforms.py` used:
- `key.endswith('_FRAC')` ‚Üí Only matched parameters ending exactly with `_FRAC`
- `key == 'BN_DUR'` ‚Üí Only matched the exact string `BN_DUR`

**Result:** Per-population bottleneck parameters were NOT transformed:
- ‚ùå `BN_TIME_FRAC_BG01` ‚Üí NOT transformed (doesn't end with `_FRAC`)
- ‚ùå `BN_SIZE_FRAC_SOUTH_LOW` ‚Üí NOT transformed
- ‚ùå `BN_DUR_PYRENEES` ‚Üí NOT transformed

This meant:
- MDN learned these in raw biological space (not unconstrained)
- Could produce negative durations
- Could produce fractions < 0 or > 1
- **Completely undermined the constraint-fixing goal of v7**

### The Fix (v8)
Changed matching logic to pattern-based:
```python
# OLD (v7):
elif key.endswith('_FRAC'):  # ‚ùå Misses BN_TIME_FRAC_BG01

# NEW (v8):
elif '_FRAC' in key:  # ‚úÖ Catches any key containing _FRAC
```

Same for durations:
```python
# OLD (v7):
elif key == 'BN_DUR':  # ‚ùå Misses BN_DUR_PYRENEES

# NEW (v8):
elif key == 'BN_DUR' or key.startswith('BN_DUR_'):  # ‚úÖ Catches all
```

**Impact:** High severity for `mode: per_population` users. Shared mode unaffected.

**Files Changed:**
- `python/npe_demography/transforms.py` (3 functions fixed)

---

## üî¥ CRITICAL FIX #2: Failed Simulations Left Zero Rows in Training Data

### The Bug (v7)
In `simulate.py`:
1. Pre-allocated memory-mapped arrays for all `n_sims`
2. Caught exceptions from failed simulations (good!)
3. But continued without marking failures
4. Saved full arrays including uninitialized/zero rows

**Result:** If 1-5% of simulations failed:
- Training data contained clusters of `(X=0, Theta=0)` fake points
- MDN learned spurious modes at zero
- Caused overconfidence in junk regions
- Distorted validation NLL
- Hidden by progress print showing `100000/100000` even with failures

### The Fix (v8)
Added success tracking:
```python
# Track which simulations succeeded
success_mask = np.zeros(n_sims, dtype=bool)
success_mask[0] = True  # Pilot

# Mark successes
for fut in as_completed(futs):
    try:
        idx, x, theta, meta = fut.result()
        # ... save data ...
        success_mask[idx] = True  # NEW
    except:
        continue  # Failure stays False

# Filter before saving
X = np.array(X_mmap[success_mask])  # Only successful sims
Theta = np.array(Theta_mmap[success_mask])
```

**Reporting improved:**
```
OLD: Progress: 100000/100000 (100.0%)  # Hides failures!
NEW: Progress: 98543/100000 succeeded (98.5%), 1457 failed
```

**Impact:** High severity. Affects all users, all modes.

**Files Changed:**
- `python/simulate.py`

---

## üî¥ CRITICAL FIX #3: Time Ordering Constraints Not Enforced in Posterior

### The Problem
The prior enforces phylogenetic time ordering:
```
T_BG01 < T_CORE < T_SOUTH_LOW < T_EAST
T_CORE < T_INT < T_CENTRAL
etc.
```

But the MDN samples times **independently** (diagonal covariance), so posterior samples can violate these constraints even though training data never did.

**Result:**
- Some posterior samples have impossible phylogenies
- Confuses interpretation
- Breaks posterior predictive checks

### The Fix (v8)
Added post-hoc filtering in `infer_posterior.py`:
```python
# Check phylogenetic constraints
valid_mask = _check_time_ordering(theta_post, param_names)

if (~valid_mask).any():
    print(f"WARNING: {(~valid_mask).sum()} samples violated constraints")
    theta_post = theta_post[valid_mask]
```

New function `_check_time_ordering()` validates all phylogenetic constraints.

**Note:** This is a **stopgap** solution. Proper fix would be to reparameterize times as cumulative gaps, but that requires reworking priors and would break backward compatibility.

**Impact:** Medium severity. Mainly affects interpretation and PPC.

**Files Changed:**
- `python/infer_posterior.py`

---

## ‚ö†Ô∏è IMPORTANT CHANGE: Increased Default n_components

### The Issue (Modeling)
With 33 parameters (per-population mode), diagonal Gaussian mixtures struggle with:
- Strong correlations between times/sizes/bottlenecks
- Ridge-like posterior geometry
- Mode dropping

With only 8 components, the MDN can:
- Underfit posterior structure
- Give overly smooth marginals
- Miss correlations

### The Fix (v8)
```yaml
# OLD (v7):
n_components: 8

# NEW (v8):
n_components: 16  # Better handles correlations
```

**Rationale:**
- 16 components gives better coverage of complex posteriors
- Minimal computational cost increase
- Still far from ideal (flows would be better), but practical improvement

**Impact:** All users benefit from better posterior quality.

**Files Changed:**
- `config/config_production.yaml`
- `config/config_pod.yaml`

---

## Summary of Changes

| Issue | Severity | Affects | Status |
|-------|----------|---------|--------|
| Per-pop params not transformed | üî¥ CRITICAL | mode: per_population | ‚úÖ FIXED |
| Failed sims poisoning training | üî¥ CRITICAL | All users | ‚úÖ FIXED |
| Time ordering not enforced | üî¥ CRITICAL | All users | ‚úÖ MITIGATED |
| MDN underfit with 8 components | ‚ö†Ô∏è IMPORTANT | All users | ‚úÖ IMPROVED |

---

## Testing

### Transform Fix Verification
```bash
cd clean_pipeline_v8
PYTHONPATH=python python3 << 'EOF'
from npe_demography.transforms import transform_to_unconstrained, inverse_transform_from_unconstrained

params = {
    'BN_TIME_FRAC_BG01': 0.4,  # Per-pop fraction
    'BN_SIZE_FRAC_SOUTH_LOW': 0.15,  # Per-pop fraction
    'BN_DUR_PYRENEES': 50.0,  # Per-pop duration
}

theta_keys = tuple(params.keys())
unc = transform_to_unconstrained(params, theta_keys)
recovered = inverse_transform_from_unconstrained(unc, theta_keys)

# Check roundtrip
for k in theta_keys:
    assert abs(params[k] - recovered[k]) < 1e-6, f"{k} failed roundtrip"
print("‚úÖ All per-population parameters transform correctly")
EOF
```

### Failed Simulation Tracking
```bash
# Run with some failures and check reporting
python python/simulate.py --config config/config_pod.yaml --n 1000

# Should see:
# "Progress: XXX/1000 succeeded (XX.X%), Y failed"
# where XXX + Y = 1000
```

### Time Ordering Check
```bash
# Run inference and check for warnings
python python/infer_posterior.py

# If violations occur, you'll see:
# "WARNING: N samples violated time ordering constraints"
```

---

## Migration Guide: v7 ‚Üí v8

### IMPORTANT: You MUST Regenerate Training Data

**v7 training data is corrupted if:**
1. You used `mode: per_population` (Issue #1 - params not transformed)
2. Any simulations failed (Issue #2 - zero rows in data)

### Step-by-Step Migration

```bash
# 1. Extract v8
tar -xzf clean_pipeline_v8.tar.gz
cd clean_pipeline_v8

# 2. Delete old training data (REQUIRED)
rm -rf simulations/sim_data.npz
rm -rf simulations/sim_part*.npz

# 3. Regenerate simulations
python python/simulate.py --config config/config_production.yaml --n 150000

# 4. Retrain model (v7 models incompatible)
python python/train_npe.py --config config/config_production.yaml

# 5. Re-infer posterior
python python/infer_posterior.py
```

### Config Changes

No config changes required. But `n_components: 16` is now default (was 8).

### Backward Compatibility

- ‚ùå v7 training data: Must regenerate
- ‚ùå v7 trained models: Must retrain
- ‚úÖ v7 configs: Work as-is
- ‚úÖ v7 observed data: Compatible
- ‚úÖ v7 scripts: Compatible

---

## Performance Impact

| Component | v7 | v8 | Change |
|-----------|----|----|--------|
| Simulation time | ~6h | ~6h | No change |
| Training time (16 components) | ~2h | ~2.5h | +25% (worth it) |
| Inference time | ~5min | ~5min | No change |
| **Total** | **~8h** | **~8.5h** | **+6%** |

The small increase is from more mixture components (16 vs 8), but this is essential for 33-parameter models.

---

## Known Limitations

### 1. Time Ordering (Partial Fix Only)

The v8 fix uses **post-hoc filtering**, which:
- ‚úÖ Prevents invalid samples in results
- ‚úÖ Warns when many samples are filtered
- ‚ùå Can bias density estimates (filtered region gets zero probability)
- ‚ùå Doesn't help MDN learn the constraints

**Better solution (future work):**
Reparameterize times as cumulative gaps:
```python
T_BG01 = log_T_BG01  # Free parameter
T_CORE = T_BG01 + softplus(gap_CORE)  # Automatically > T_BG01
T_SOUTH_LOW = T_CORE + softplus(gap_SOUTH_LOW)  # Automatically > T_CORE
...
```

This would **enforce** ordering in the parameter space itself.

### 2. Diagonal Covariance Still Limited

Even with 16 components, diagonal Gaussian mixtures cannot perfectly represent:
- Strong parameter correlations (e.g., Ne-T tradeoffs)
- Ridge-like posteriors
- Non-Gaussian geometries

**Better solution (future work):**
- Use normalizing flows (NSF/MAF)
- Sequential neural posterior estimation (SNPE)
- Full-rank Gaussian mixtures

### 3. Simulation Failures Not Debugged

v8 **correctly handles** failed simulations, but doesn't tell you **why** they failed.

**Recommendations:**
- Monitor failure rate (should be < 2%)
- If > 5%, investigate:
  - SLiM errors in worker logs
  - Prior bounds too wide (extreme parameters)
  - Timeout too short for complex simulations

---

## Validation Checklist

After migrating to v8, verify:

**1. Transformations Working:**
```bash
PYTHONPATH=python python scripts/test_transforms.py
# Should see: "ALL TESTS PASSED ‚úÖ"
```

**2. No Failed Simulations Poisoning Data:**
```bash
# Check simulation output for success rate
python python/simulate.py ... | grep "succeeded"
# Should see: "98543/100000 succeeded (98.5%)" or similar
# Anything < 95% needs investigation
```

**3. Time Ordering Enforced:**
```bash
# Check inference output for violations
python python/infer_posterior.py | grep "WARNING"
# Ideally see no warnings, but < 5% violations is acceptable
```

**4. POD Recovery:**
```bash
bash scripts/run_pod_test.sh
python scripts/check_pod_recovery.py
# Should see ~95% coverage
```

---

## Credit

These bugs were identified by careful code review. Thank you to the reviewer for the detailed analysis!

---

## Next Steps After v8

### Short-term
1. Validate with POD test
2. Run production analysis
3. Check for time ordering violations
4. Monitor failure rates

### Medium-term
1. Implement cumulative-gap time parameterization
2. Increase n_components to 24-32 for very complex models
3. Add SBC (simulation-based calibration) validation

### Long-term
1. Move to normalizing flows for better correlation handling
2. Implement sequential SNPE for efficiency
3. Add hierarchical priors for per-population parameters

---

**v8 fixes critical correctness issues. All users should upgrade immediately.**
